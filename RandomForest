随机森林是集成学习的主要代表作，它是以bagging算法为基础的加法模型。所以在学习随机森林之前先了解一下bagging算法。



一、bagging原理

bagging是集成学习三大基本模式（bagging， stacking和boosting）之一。
bagging基本原理：
    1.假设样本集{（xi, yi）} i = 1,...N， 特征矩阵X有H个特征;
    2.随机从样本集中随机抽取N个样本组成样本集Dm；（该抽取过程是放回的）
    3.基于样本集Dm训练模型；
    4.重复2、3步骤迭代M轮，平均模型f（x）得到模型F(x)；





    假设样本集为{（xi，yi）} i = 1,...N. 对于随机抽样部分，我们了解到抽样是放回的，一个样本被抽到的概率是1/N,
不被抽中的概率是（1-1/N）,所以总的不被抽中的概率（1-1/N）^N, 当N无穷大的时候，概率趋近1/e。样本集中不被抽中的
样本数大约为N*1/e,这又被称为袋外数据（out of bag，简称OOB）。

二、随机森林基本原理

随机森林：我们从字面意思上理解，该学习模型具有随机性且由多棵决策树构成。随机性主要体现在随机抽样（行随机）和随机
选择特征（列随机）两方面，可以很好地防止过拟合现象发生；多棵决策树可以防止模型泛化能力低现象发生，这些现象在我们
平常做研究的时候是很常见的。

随机森林流程：

假设数据集D = {（xi，yi）} i = 1,...N. 特征空间X特征的个数为H，迭代M轮，第j轮随机抽样形成的样本集Dj，第H轮训
练的回归分类器（基函数）为Gj；

for j to M do:

1、从D中随机抽样得到抽样样本集Dj；

2、根据Dj训练CART得到回归分类器Gj；

3、如果是回归问题，F（x） += Gj / M；

endfor

4、得到模型F（x） 



根据Dj训练CART得到回归分类器Gj：

1、随机选择hj个特征（hj <<H）,这里这个特征也是放回的，也就意味着树之间可能有共同的特征；

2、根据随机选的hj个特征和CART生成树准则训练回归分类树，截至条件是结点上的样本都分成了一类或者分裂时两个特征
连续出现；

随机选择hj个特征，相对于使用全部特征而言，树的方差减小有效防止过拟合现象， 但偏差会增大，所以hj选择必须得当，
可以用搜索法和交叉验证得到hj。



随机层林的优缺点：

优点：

1、由随机森林基本原理，我们可以发现基模型之间的独立性很强，所以该算法可以并行处理， 也就是基于随机森林和大数
据平台可以很好地应对大数据时代的到来。

2、随机抽样和随机抽取特征可以很好地防止过拟合现象；

3、随机抽取特征也可以克服特征维度过高问题；

4、模型结构相对简单；



缺点：

1、对于数据噪音过大的样本集，容易产生过你现象；

2、对于有不同取值的属性的数据，取值划分较多的属性会对随机森林产生更大的影响，所以随机森林在这种数据上产出的属
性权值是不可信的。（不太明白）

参考：

https://www.cnblogs.com/pinard/p/6156009.html

https://www.cnblogs.com/bugsheep/p/7898320.html




